#!/usr/bin/env python3

import fsmodule.extract_multigene_read as extract_multigene_read
import fsmodule.construct_gtf as construct_gtf
import fsmodule.stat_split_reads as stat_split_reads
import fsmodule.cluster as cluster
import fsmodule.align as align
import fsmodule.annotate_fusion as annotate_fusion
from multiprocessing import Pool
import argparse
import pysam
import gzip
import sys
import os
import setproctitle
from tqdm import tqdm
import time
setproctitle.setproctitle("FusionSeekerPro")

def parse_args() -> argparse.Namespace:
	parser=argparse.ArgumentParser(description='Gene fusion caller for long-read sequencing data', usage='fusionseeker [-h] --bam <sort.bam>')
	parser.add_argument('-v','--version', action='version', version='FusionSeekerPro v1.1.0')
	parser.add_argument('--bam',type=str,default=False,required=True,help='Input sorted BAM. index required')
	parser.add_argument('--datatype',type=str,default=False,help='Input read type (isoseq, nanopore) [nanopore]')
	parser.add_argument('--gtf',type=str,default=False,help='Genome annotation file')
	parser.add_argument('--ref',type=str,default=None,help='Reference genome. Required for breakpoint polishing')
	parser.add_argument('--geneid',action='store_true',default=False,help='Use Gene ID instead of Gene name [False]')
	parser.add_argument('--human38',action='store_true',default=False,help='Use reference genome and GTF for Human GCRh38 (default)')
	parser.add_argument('--human19',action='store_true',default=False,help='Use reference genome and GTF for Human GCRh37')

	parser.add_argument('-o','--outpath',type=str,default='./fusionseeker_out/',help='Output directory [./fusionseeker_out/]')
	parser.add_argument('-s','--minsupp',type=int,default=None,help='Minimal reads supporting an event [auto]')

	parser.add_argument('--maxdistance',type=int,default=False,help='Maximal distance to cluster raw signals [20 for isoseq, 40 for nanopore]')
	parser.add_argument('--keepfile',action='store_true',default=False,help='Keep intermediate files [False]')
	parser.add_argument('--redo',action='store_true',default=False,help='Redo the analysis from the beginning [False]')
	parser.add_argument('--skipsupplementary',action='store_true',default=False,help='Skip the raw signal detection of supplementary alignments [False]')
	parser.add_argument('--thread',type=int,default=8,help='Number of threads [8]')
	parser.add_argument('--windowsize',type=int,default=10000000,help='Window size for task splitting [1000000]')

	args=parser.parse_args()

	return args

def create_genomic_regions(bamfile, chrom_valid, windowsize=1000000):
    """
    split genomic regions into tasks
    Args:
        bamfile: pysam.AlignmentFile object
        chrom_valid: valid chromosome list
        windowsize: window size for task splitting
    Returns:
        list: [(chrom, start, end), ...] genomic region list
    """
    regions = []
    for chrom in chrom_valid:
        chrom_length = bamfile.get_reference_length(chrom)
        start = 0
        while start < chrom_length:
            end = min(start + windowsize, chrom_length)
            regions.append((chrom, start, end))
            start = end
    return regions

def main():
	args = parse_args()

	# create output directory
	if args.outpath[-1] != '/':
		args.outpath += '/'
	if os.path.exists(args.outpath) and args.redo:
		os.system('rm -rf ' + args.outpath)
	os.makedirs(args.outpath, exist_ok=True)

	# write log file
	with open(args.outpath+'log.txt', 'w') as logfile:
		logfile.write(f'[INFO] Start calling gene fusions from {args.bam}...\n')

	# check datatype
	if args.datatype not in ['isoseq', 'nanopore']:
		with open(args.outpath + 'log.txt', 'a') as logfile:
			logfile.write('[WARNING] --datatype has to be one of the following: isoseq, nanopore. Using nanopore by default.\n')
		args.datatype = 'nanopore'

	if not args.maxdistance:
		if args.datatype == 'isoseq':
			args.maxdistance = 20
		else:
			args.maxdistance = 40

	bamfile = pysam.AlignmentFile(args.bam, 'rb')
	bamchrom = bamfile.references

    # use default GTF file if not provided
	if not args.gtf:
		fusionseekerpath = os.path.dirname(__file__)
		if fusionseekerpath != '':
			fusionseekerpath += '/'
		if args.human19:
			args.gtf = fusionseekerpath + 'data/Homo_sapiens.GRCh37.87.chrname.gtf.gz'
		else:
			args.gtf = fusionseekerpath + 'data/Homo_sapiens.GRCh38.104.chrname.gtf.gz'

    # read GTF file
	try:
		raw_gtfinfo = gzip.open(args.gtf, 'rt').read().split('\n')[:-1]
	except:
		raw_gtfinfo = open(args.gtf, 'r').read().split('\n')[:-1]

	# parse GTF file
	gtfinfo = {}
	for line in raw_gtfinfo:
		if line[0] == '#':
			continue
		if line.split('\t')[0] not in gtfinfo.keys():
			gtfinfo[line.split('\t')[0]] = []
		gtfinfo[line.split('\t')[0]].append(line)
	
	allchrom=list(gtfinfo.keys())

	# check if all chromosomes from GTF file are present in BAM file
	chrom_valid=[c for c in allchrom if c in bamchrom]
	chrom_not_in_bam=[c for c in allchrom if c not in bamchrom]
	chrom_not_in_gtf=[c for c in bamchrom if c not in allchrom]
	if len(chrom_not_in_bam) > 0:
		with open(args.outpath+'log.txt', 'a') as logfile:
			logfile.write('[WARNING] Following chromosomes are not in BAM file, skipping '+';'.join(chrom_not_in_bam)+'\n')
	if len(chrom_not_in_gtf)>0:
		with open(args.outpath+'log.txt', 'a') as logfile:
			logfile.write('[WARNING] Following chromosomes in BAM file are not in GTF file, skipping '+';'.join(chrom_not_in_gtf)+'\n')
	if len(chrom_valid)==0:
		with open(args.outpath+'log.txt', 'a') as logfile:
			logfile.write('[ERROR] None of the chromosomes from GTF file are present in BAM. Check if chromosome names match between BAM and GTF. Abort.\n')
		sys.exit(1)

	"""
	Step 0: Extract multi-gene reads
	"""
	if not os.path.exists(args.outpath + 'filtered.bam') or args.redo:
		extract_multigene_read.extract_multigene_read(args.bam, args.gtf, args.outpath, args.skipsupplementary)
		print("Multigene reads extracted.")
	else:
		print("Multigene reads already extracted. Skipping.")
		with open(args.outpath + 'log.txt', 'a') as logfile:
			logfile.write('[INFO] Multigene reads already extracted. Skipping.\n')

	"""
	Step 1: Construct gene info
	"""
	# [0] start position [1] end position [2] gene name [3] strand [4] exon positions
	geneinfo = construct_gtf.create(gtfinfo, chrom_valid, args.geneid, args.outpath, writedata = True)
	
	# Create gene2strand mapping
	gene2strand = {}
	for chrom in geneinfo:
		for gene in geneinfo[chrom]:
			gene2strand[gene[2]] = gene[3]
	print("Gene info constructed.")

	"""
	Step 2: Raw signal detection
	"""
	stat_split_reads.geneinfo = geneinfo
	stat_split_reads.gene2strand = gene2strand  # Set as module-level variable for multiprocessing
	genomic_regions = create_genomic_regions(bamfile, chrom_valid, args.windowsize)
	print("Genomic regions created.")

	# 2a. compute raw signal from each read
	if not os.path.exists(args.outpath+'raw_signal/') or not os.path.exists(args.outpath+'rawsignal.txt') or args.redo:
		os.makedirs(args.outpath+'raw_signal/', exist_ok=True)
		results = []
		with open(args.outpath+'log.txt', 'a') as logfile:
			logfile.write(f'[INFO] {len(genomic_regions)} genomic regions created (window size: {args.windowsize/1e6} Mbp).\n')

		pool = Pool(args.thread)
		for chrom, start, end in genomic_regions:
			res = pool.apply_async(stat_split_reads.get_split_reads,
									args=(f'{args.outpath}filtered.bam', args.outpath, chrom, start, end))
			results.append(res)
		pool.close()
		"""
		for res in results:
			try:
				res.get() # there will be error if the child process fails
			except Exception as e:
				with open(args.outpath+'log.txt', 'a') as logfile:
					logfile.write(f"[ERROR] Error in worker process: {e}\n")
				raise Exception(f"Error in worker process: {e}")
		"""
		
		# Use tqdm to show progress - process completed tasks first for real-time updates
		completed = set()
		with tqdm(total=len(genomic_regions), desc="Processing genomic regions") as pbar:
			while len(completed) < len(results):
				found_completed = False
				for i, res in enumerate(results):
					if i not in completed and res.ready():
						try:
							res.get()  # Get result (may raise exception)
							completed.add(i)
							pbar.update(1)
							found_completed = True
						except Exception as e:
							with open(args.outpath + 'log.txt', 'a') as logfile:
								logfile.write(f"[ERROR] Error in worker process: {e}\n")
							raise Exception(f"Error in worker process: {e}")
				if not found_completed:
					time.sleep(0.1)  # Avoid CPU spinning when no tasks are ready

		pool.join()

		# 2b. reanalyze raw signal from reads with supplementary alignments
		if args.skipsupplementary:
			print(f"Supplementary alignments skipped. Skipping. (To reanalyze the raw signal from reads with supplementary alignments, please use --skiprawsignal flag.)")
			with open(args.outpath + 'log.txt','a') as logfile:
				logfile.write('[INFO] Supplementary alignments skipped. Skipping.\n')
		else:
			supplementary_read_info = {}
			for chrom, start, end in genomic_regions:
				with open(f"{args.outpath}raw_signal/multigene_read_{chrom}_{start}_{end}.txt", 'r') as fi:
					for line in fi:
						if line.startswith('chrom'):
							continue
						readinfo = line.strip().split('\t') # chrom, start, end, read_name, strand, is_supplementary, general_alignment_info, mapq, nmrate, left_gene, right_gene, exon_info, cigar, sequence, quality
						readinfo[1], readinfo[2], readinfo[5] = int(readinfo[1]), int(readinfo[2]), bool(readinfo[5])
						if readinfo[5]: # is supplementary alignment
							if readinfo[3] not in supplementary_read_info.keys():
								supplementary_read_info[readinfo[3]] = []
							supplementary_read_info[readinfo[3]].append(readinfo[0:3])

			pool = Pool(args.thread)
			results = []
			for read_name, read_pos_list in supplementary_read_info.items():
				res = pool.apply_async(stat_split_reads.reanalyze_supplementary_alignment,
										args=(args.bam, args.outpath, read_name, read_pos_list))
				results.append(res)
			pool.close()
		
			"""
			for res in results:
				try:
					res.get() # there will be error if the child process fails
				except Exception as e:
					with open(args.outpath+'log.txt', 'a') as logfile:
						logfile.write(f"[ERROR] Error in worker process: {e}\n")
					raise Exception(f"Error in worker process: {e}")
			"""
			
			# Use tqdm to show progress - process completed tasks first for real-time updates
			completed = set()
			with tqdm(total=len(supplementary_read_info), desc="Reanalyzing supplementary alignments") as pbar:
				while len(completed) < len(results):
					found_completed = False
					for i, res in enumerate(results):
						if i not in completed and res.ready():
							try:
								res.get()  # Get result (may raise exception)
								completed.add(i)
								pbar.update(1)
								found_completed = True
							except Exception as e:
								with open(args.outpath + 'log.txt', 'a') as logfile:
									logfile.write(f"[ERROR] Error in worker process: {e}\n")
								raise Exception(f"Error in worker process: {e}")
					if not found_completed:
						time.sleep(0.1)  # Avoid CPU spinning when no tasks are ready
			pool.join()

		print("Raw signal detection done.")
		with open(args.outpath + 'log.txt','a') as logfile:
			logfile.write('[INFO] Raw signal detection done. Start clustering raw signals...\n')

	else:
		print(f"Raw signal detection already done. Skipping. (To redo the raw signal detection, please use --redo flag.)")
		with open(args.outpath + 'log.txt', 'a') as logfile:
			logfile.write('[INFO] Raw signal detection already done. Skipping.\n')

	"""
	Step 3: Clustering raw signals
	"""
	if not os.path.exists(args.outpath+'confident_genefusion.txt') or args.redo:
		cluster.cluster_bp(args.outpath, args.maxdistance, args.minsupp)
		
		print("Raw signal clustering done.")
		with open(args.outpath + 'log.txt', 'a') as logfile:
			logfile.write('[INFO] Raw signal clustering done. Start transcript sequence generation...\n')
	else:
		print(f"Raw signal clustering already done. Skipping. (To redo the raw signal clustering, please use --redo flag.)")
		with open(args.outpath + 'log.txt', 'a') as logfile:
			logfile.write('[INFO] Raw signal clustering already done. Skipping.\n')

	"""
	Step 4: Transcript sequence generation
	"""
	if not os.path.exists(args.outpath+'confident_genefusion_refined_transcript_sequence.fa') or args.redo:
		align.geneinfo = geneinfo
		align.poa_all(args.outpath, genomic_regions)
		with open(args.outpath+'log.txt','a') as logfile:
			if args.ref:
				try:
					align.polish_bp(args.outpath, genomic_regions, args.ref, args.datatype, True, args.skipsupplementary)
					logfile.write('[INFO] Transcript sequence generation done. Start polishing gene fusion breakpoints...\n')
				except Exception as e:
					logfile.write(f'[WARNING] Transcript sequence generation failed: {e}\n')
					os.system(f'touch {args.outpath}confident_genefusion_refined_transcript_sequence.fa')
			else:
				try:
					align.polish_bp(args.outpath, genomic_regions, args.ref, args.datatype, False, args.skipsupplementary)
					logfile.write('[INFO] Transcript sequence generation done.\n[WARNING] No reference genome provided. Skipping breakpoint polish. (To enable breakpoint polish, please provide reference genome file.)\n')
				except Exception as e:
					logfile.write(f'[WARNING] Transcript sequence generation failed: {e}\n')
					os.system(f'touch {args.outpath}confident_genefusion_refined_transcript_sequence.fa')

		print("Transcript sequence generation done.")
		with open(args.outpath + 'log.txt','a') as logfile:
			logfile.write('[INFO] Transcript sequence generation done.\n')
	else:
		print(f"Transcript sequence generation already done. Skipping. (To redo the transcript sequence generation, please use --redo flag.)")
		with open(args.outpath + 'log.txt', 'a') as logfile:
			logfile.write('[INFO] Transcript sequence generation already done. Skipping.\n')

	"""
	Step 5: Gene structure annotation and fusion type classification
	"""
	is_annotated = False
	with open(args.outpath + 'confident_genefusion_refined.txt', 'r') as fi:
		first_line = fi.readline()
		if 'exonNum' in first_line: # exonNum, exonType, fusionType
			is_annotated = True
	
	if not is_annotated or args.redo:
		annotate_fusion.annotate_fusion(args.outpath)
		print("Gene structure annotation and fusion type classification done.")
		with open(args.outpath + 'log.txt','a') as logfile:
			logfile.write('[INFO] Gene structure annotation and fusion type classification done.\n')
	else:
		print(f"Gene structure annotation and fusion type classification already done. Skipping. (To redo, please use --redo flag.)")
		with open(args.outpath + 'log.txt', 'a') as logfile:
			logfile.write('[INFO] Gene structure annotation and fusion type classification already done. Skipping.\n')

	# remove intermediate files
	if not args.keepfile:
		os.system(f'rm -r {args.outpath}align_workspace/')
		os.system(f'rm -r {args.outpath}raw_signal/')
		print("Intermediate files removed.")
		with open(args.outpath + 'log.txt','a') as logfile:
			logfile.write('[INFO] Intermediate files removed.\n')

	print("All done. Bye.")
	with open(args.outpath + 'log.txt','a') as logfile:
		logfile.write('[INFO] Gene fusion detection done. Bye.\n')

if __name__ == '__main__':
	main()
